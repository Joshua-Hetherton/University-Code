{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent Agents #\n",
    "\n",
    "This notebook serves as supporting material for topics covered in **Chapter 2 - Intelligent Agents** from the book *Artificial Intelligence: A Modern Approach.* \n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- The formal definition of an intelligent agent and its components\n",
    "- How agents perceive and act in environments\n",
    "- The agent-environment interaction loop\n",
    "- How to implement simple and complex agent behaviors\n",
    "- The progression from reactive to more sophisticated agent architectures\n",
    "\n",
    "This notebook uses implementations from [agents.py](https://github.com/aimacode/aima-python/blob/master/agents.py) module. Let's start by importing everything from agents module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import *\n",
    "from notebook import psource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTENTS\n",
    "\n",
    "* Overview\n",
    "* Agent\n",
    "* Environment\n",
    "* Simple Agent and Environment\n",
    "* Agents in a 2-D Environment\n",
    "* Wumpus Environment\n",
    "\n",
    "## OVERVIEW\n",
    "\n",
    "### What is an Intelligent Agent?\n",
    "\n",
    "An **agent**, as defined in Section 2.1 of AIMA, is anything that can:\n",
    "1. **Perceive** its environment through **sensors**\n",
    "2. **Act** upon that environment through **actuators** \n",
    "3. Make decisions based on its **agent program**\n",
    "\n",
    "**Mathematical Definition**: An agent can be described as a function that maps percept sequences to actions:\n",
    "```\n",
    "f: P* → A\n",
    "```\n",
    "Where P* is the set of all possible percept sequences, and A is the set of possible actions.\n",
    "\n",
    "### Real-World Examples\n",
    "- **Humans**: Eyes/ears (sensors) → Brain (program) → Hands/voice (actuators)\n",
    "- **Robots**: Cameras/LIDAR (sensors) → CPU (program) → Motors/servos (actuators)  \n",
    "- **Software agents**: Network data (sensors) → Algorithm (program) → API calls (actuators)\n",
    "\n",
    "### The Agent-Environment Loop\n",
    "```\n",
    "Environment → Percepts → Agent → Actions → Environment\n",
    "```\n",
    "\n",
    "This notebook will demonstrate how to implement this fundamental AI concept step by step, starting with simple reactive agents and progressing to more complex behaviors.\n",
    "\n",
    "## AGENT\n",
    "\n",
    "Let us now see how we define an agent. Run the next cell to see how `Agent` is defined in agents module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psource(Agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Agent Class Structure\n",
    "\n",
    "The `Agent` class provides the foundation for all intelligent agents. Let's examine its key components:\n",
    "\n",
    "#### Constructor: `__init__(self, program=None)`\n",
    "The constructor initializes the agent's **internal state** with these crucial attributes:\n",
    "\n",
    "* **`alive`**: Tracks agent vitality (important for survival-based environments)\n",
    "* **`bump`**: Collision detection (helps agent understand environment boundaries)  \n",
    "* **`holding`**: Memory of carried objects (enables goal-directed behavior)\n",
    "* **`performance`**: Quantitative evaluation metric (measures agent effectiveness)\n",
    "* **`program`**: The \"brain\" of the agent - maps percepts to actions\n",
    "\n",
    "> **Key Insight**: The `program` parameter is the most critical - it defines the agent's **behavior policy**. If no program is provided, the agent will ask for human input (manual control mode).\n",
    "\n",
    "#### Method: `can_grab(self, thing)`\n",
    "This method defines the agent's **physical capabilities** - what objects it can interact with in the environment. By default, agents cannot carry anything (like our \"blind dog\" example).\n",
    "\n",
    "### Why These Attributes Matter\n",
    "These attributes represent the **PEAS framework** components:\n",
    "- **P**erformance: `performance` attribute\n",
    "- **E**nvironment: Handled by separate Environment class  \n",
    "- **A**ctuators: Methods like movement and actions\n",
    "- **S**ensors: `percept()` method input\n",
    "\n",
    "## ENVIRONMENT\n",
    "Now, let us see how environments are defined. Running the next cell will display an implementation of the abstract `Environment` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psource(Environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Environment Class\n",
    "\n",
    "The `Environment` class models the world in which our agents operate. It has many methods, but let's focus on the essential ones for understanding agent-environment interaction:\n",
    "\n",
    "#### Core Environment Methods:\n",
    "\n",
    "* **`thing_classes(self)`**: Defines what types of objects can exist in this environment (environment constraints)\n",
    "\n",
    "* **`add_thing(self, thing, location=None)`**: Places objects in the environment at specific locations\n",
    "\n",
    "* **`run(self, steps)`**: **The main simulation loop** - executes the agent-environment interaction for a specified number of time steps\n",
    "\n",
    "* **`is_done(self)`**: Termination condition - determines when the simulation should end\n",
    "\n",
    "#### Critical Abstract Methods (Must Be Implemented):\n",
    "Every Environment subclass **must** implement these two methods to enable the agent-environment loop:\n",
    "\n",
    "* **`percept(self, agent)`**: **Sensor function** - returns what the agent can \"see\" at its current location\n",
    "  - Input: The agent requesting percepts\n",
    "  - Output: List of perceivable objects/stimuli\n",
    "  - This is the agent's \"sensory input\"\n",
    "\n",
    "* **`execute_action(self, agent, action)`**: **Actuator function** - processes agent actions and updates world state  \n",
    "  - Input: The agent and its chosen action\n",
    "  - Output: Changes to environment state\n",
    "  - This is how agent actions \"affect the world\"\n",
    "\n",
    "### The Agent-Environment Interaction Cycle:\n",
    "```\n",
    "1. Environment calls agent.program(percepts)\n",
    "2. Agent.program returns an action  \n",
    "3. Environment.execute_action(agent, action) \n",
    "4. Environment state changes\n",
    "5. Repeat until is_done() returns True\n",
    "```\n",
    "\n",
    "This cycle implements the fundamental **perception-action loop** that drives all intelligent behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIMPLE AGENT AND ENVIRONMENT\n",
    "\n",
    "Let's implement our first intelligent agent using the **Simple Reflex Agent** architecture from AIMA Chapter 2. \n",
    "\n",
    "### Why a \"Blind Dog\"?\n",
    "We're creating a \"blind dog\" agent to illustrate key AI concepts:\n",
    "- **Limited perception**: Can only sense objects at its exact location (like touch)\n",
    "- **Simple behavior**: Basic stimulus-response reactions  \n",
    "- **Goal-directed**: Seeks food and water (survival needs)\n",
    "\n",
    "This demonstrates a **reactive agent** - one that chooses actions based only on current percepts, without memory of past states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlindDog(Agent):\n",
    "    def eat(self, thing):\n",
    "        # Action: Consume food when available\n",
    "        print(\"Dog: Ate food at {}.\".format(self.location))\n",
    "            \n",
    "    def drink(self, thing):\n",
    "        # Action: Consume water when available  \n",
    "        print(\"Dog: Drank water at {}.\".format( self.location))\n",
    "\n",
    "# Create our first agent instance\n",
    "dog = BlindDog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent State Inspection\n",
    "\n",
    "What we have just created is a dog agent who can only perceive objects at its exact location (since it's \"blind\" to distant objects), and can perform eating or drinking actions. \n",
    "\n",
    "Let's check if our agent is alive and ready for action..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the agent's initial state\n",
    "print(dog.alive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cool dog](https://gifgun.files.wordpress.com/2015/07/wpid-wp-1435860392895.gif)\n",
    "This is our dog. How cool is he? Well, he's hungry and needs to go search for food. For him to do this, we need to give him a program. But before that, let's create a park for our dog to play in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENVIRONMENT - Park\n",
    "\n",
    "Now we need to create an **environment** for our agent to inhabit. A park serves as an excellent example because:\n",
    "\n",
    "1. **Observable**: Our dog can sense objects at its location\n",
    "2. **Deterministic**: Actions have predictable results  \n",
    "3. **Sequential**: Actions affect future percepts\n",
    "4. **Dynamic**: Objects can be consumed (food/water disappears)\n",
    "\n",
    "Since `Environment` is an abstract class, we must create our own concrete subclass that implements the required methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objects that can exist in our environment\n",
    "class Food(Thing):\n",
    "    pass\n",
    "\n",
    "class Water(Thing):\n",
    "    pass\n",
    "\n",
    "class Park(Environment):\n",
    "    def percept(self, agent):\n",
    "        '''SENSOR FUNCTION: Return list of objects at agent's current location\n",
    "        This implements the agent's sensory capabilities - what it can \"feel\" or detect'''\n",
    "        things = self.list_things_at(agent.location)\n",
    "        return things\n",
    "    \n",
    "    def execute_action(self, agent, action):\n",
    "        '''ACTUATOR FUNCTION: Process agent's action and update environment state\n",
    "        This implements how agent actions change the world'''\n",
    "        if action == \"move down\":\n",
    "            print('{} decided to {} at location: {}'.format(str(agent)[1:-1], action, agent.location))\n",
    "            agent.movedown()\n",
    "        elif action == \"eat\":\n",
    "            # Find food at current location\n",
    "            items = self.list_things_at(agent.location, tclass=Food)\n",
    "            if len(items) != 0:\n",
    "                if agent.eat(items[0]): # Agent attempts to eat first food item\n",
    "                    print('{} ate {} at location: {}'\n",
    "                          .format(str(agent)[1:-1], str(items[0])[1:-1], agent.location))\n",
    "                    self.delete_thing(items[0]) # Remove consumed food from environment\n",
    "        elif action == \"drink\":\n",
    "            # Find water at current location\n",
    "            items = self.list_things_at(agent.location, tclass=Water)\n",
    "            if len(items) != 0:\n",
    "                if agent.drink(items[0]): # Agent attempts to drink first water item\n",
    "                    print('{} drank {} at location: {}'\n",
    "                          .format(str(agent)[1:-1], str(items[0])[1:-1], agent.location))\n",
    "                    self.delete_thing(items[0]) # Remove consumed water from environment\n",
    "\n",
    "    def is_done(self):\n",
    "        '''TERMINATION CONDITION: Simulation ends when no food/water remains or agent dies\n",
    "        This prevents our cute dog from starving by ending simulation before resources are exhausted'''\n",
    "        no_edibles = not any(isinstance(thing, Food) or isinstance(thing, Water) for thing in self.things)\n",
    "        dead_agents = not any(agent.is_alive() for agent in self.agents)\n",
    "        return dead_agents or no_edibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROGRAM - BlindDog Enhanced\n",
    "\n",
    "Now that we have a `Park` environment, we need to enhance our `BlindDog` with:\n",
    "1. **Mobility**: Ability to move through the environment\n",
    "2. **Decision-making**: Logic to determine when eating/drinking is successful\n",
    "3. **Spatial awareness**: Track location in the 1D park space\n",
    "\n",
    "This represents an upgrade from a purely reactive agent to one with basic **state-based behavior**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlindDog(Agent):\n",
    "    location = 1  # Agent's position in 1D space (starting location)\n",
    "    \n",
    "    def movedown(self):\n",
    "        # ACTUATOR: Move to next location in linear space\n",
    "        self.location += 1\n",
    "        \n",
    "    def eat(self, thing):\n",
    "        '''EATING BEHAVIOR: Attempt to consume food\n",
    "        Returns True if successful, False otherwise'''\n",
    "        if isinstance(thing, Food):\n",
    "            return True  # Successfully ate food\n",
    "        return False     # Cannot eat non-food items\n",
    "    \n",
    "    def drink(self, thing):\n",
    "        '''DRINKING BEHAVIOR: Attempt to consume water\n",
    "        Returns True if successful, False otherwise'''\n",
    "        if isinstance(thing, Water):\n",
    "            return True  # Successfully drank water\n",
    "        return False     # Cannot drink non-water items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Agent Program - The \"Brain\" of Our Agent\n",
    "\n",
    "Now we implement the **agent program** - the core decision-making function that maps percepts to actions. This represents the agent's **policy** or **behavior strategy**.\n",
    "\n",
    "Our program implements a **Simple Reflex Agent** with the following condition-action rules:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><b>Percept:</b> </td>\n",
    "        <td>Feel Food </td>\n",
    "        <td>Feel Water</td>\n",
    "        <td>Feel Nothing</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "       <td><b>Action:</b> </td>\n",
    "       <td>eat</td>\n",
    "       <td>drink</td>\n",
    "       <td>move down</td>\n",
    "   </tr>\n",
    "</table>\n",
    "\n",
    "**Key Insight**: This is a **greedy** policy - the agent always chooses immediate consumption over exploration. This works well when resources are guaranteed to be found by linear search.\n",
    "\n",
    "**Algorithm Type**: This implements a simple **lookup table** or **condition-action rules** - the most basic form of AI decision making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def program(percepts):\n",
    "    '''AGENT PROGRAM: The decision-making function that maps percepts to actions\n",
    "    This is the \"brain\" of our agent - implements Simple Reflex Agent behavior\n",
    "    \n",
    "    Args:\n",
    "        percepts: List of objects/stimuli the agent can sense at current location\n",
    "    Returns:\n",
    "        action: String representing the chosen action\n",
    "    '''\n",
    "    # Priority 1: Consume resources if available (survival behavior)\n",
    "    for p in percepts:\n",
    "        if isinstance(p, Food):\n",
    "            return 'eat'      # Immediate consumption of food\n",
    "        elif isinstance(p, Water):\n",
    "            return 'drink'    # Immediate consumption of water\n",
    "    \n",
    "    # Priority 2: Explore environment if no resources detected  \n",
    "    return 'move down'        # Continue searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Our First Agent Simulation\n",
    "\n",
    "Now let's create a complete agent-environment system and observe the **perception-action loop** in action. We'll set up a scenario with strategically placed resources to test our agent's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMULATION SETUP: Create environment and populate with agent and resources\n",
    "park = Park()\n",
    "dog = BlindDog(program)   # Attach our decision-making program to the agent\n",
    "dogfood = Food()\n",
    "water = Water()\n",
    "\n",
    "# ENVIRONMENT INITIALIZATION: Place objects at specific locations\n",
    "park.add_thing(dog, 1)      # Dog starts at location 1\n",
    "park.add_thing(dogfood, 5)  # Food at location 5  \n",
    "park.add_thing(water, 7)    # Water at location 7\n",
    "\n",
    "# RUN SIMULATION: Execute 5 time steps of agent-environment interaction\n",
    "park.run(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Agent Behavior\n",
    "\n",
    "**Observation**: The dog moved from location 1 → 4 over 4 steps, then ate food at location 5 in the 5th step.\n",
    "\n",
    "**Analysis**: This demonstrates the **exploration-exploitation trade-off**:\n",
    "- **Exploration phase**: Steps 1-4, agent searches environment  \n",
    "- **Exploitation phase**: Step 5, agent consumes discovered resource\n",
    "\n",
    "Let's continue the simulation to see what happens next..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTINUE SIMULATION: Run 5 more steps to observe complete behavior\n",
    "park.run(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Simulation Termination\n",
    "\n",
    "**Perfect!** Notice how the simulation **automatically stopped** after the dog consumed the water. This demonstrates an important AI concept: **goal achievement** and **termination conditions**.\n",
    "\n",
    "**Why did it stop?** Our `is_done()` method detected that all consumable resources were exhausted, meeting our defined success criteria.\n",
    "\n",
    "Let's test the agent's **persistence** by adding more resources and observing continued exploration..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DYNAMIC ENVIRONMENT: Add resources during simulation to test adaptability\n",
    "park.add_thing(water, 15)  # Place water at distant location 15\n",
    "park.run(10)               # Run longer simulation to test persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Learnings from 1D Agent Simulation\n",
    "\n",
    "We've successfully implemented and tested a **Simple Reflex Agent** that demonstrates:\n",
    "1. **Perception-Action Loop**: Continuous sensing and responding\n",
    "2. **Goal-Directed Behavior**: Seeking survival resources  \n",
    "3. **Environment Interaction**: Modifying world state through actions\n",
    "4. **Termination Conditions**: Recognizing task completion\n",
    "\n",
    "However, this was a simplified case with **limited complexity**. Real-world AI agents face more challenging environments!\n",
    "\n",
    "## AGENTS IN A 2D ENVIRONMENT\n",
    "\n",
    "### Why Upgrade to 2D?\n",
    "\n",
    "Moving from 1D to 2D space introduces several important AI challenges:\n",
    "\n",
    "1. **Increased State Space**: Exponentially more possible locations\n",
    "2. **Navigation Complexity**: Need for spatial reasoning and pathfinding  \n",
    "3. **Directional Awareness**: Agents must track orientation (North, South, East, West)\n",
    "4. **Boundary Handling**: Collision detection and environment limits\n",
    "5. **Visual Representation**: Graphics help us understand agent behavior\n",
    "\n",
    "### Introducing GraphicEnvironment\n",
    "\n",
    "To visualize our 2D world, we'll use `GraphicEnvironment` which adds:\n",
    "\n",
    "- **Coordinate System**: Standard X-Y plane positioning (4th quadrant indexing)\n",
    "- **Color Coding**: RGB visual representation of objects (Red=Agent, Blue=Water, Orange=Food)  \n",
    "- **Boundary Fencing**: Automatic collision detection with environment edges\n",
    "- **Real-time Visualization**: Watch agents move and interact graphically\n",
    "\n",
    "**Safety Feature**: The `is_inbounds()` function prevents our blind dog from wandering outside the park - it's dangerous out there!\n",
    "\n",
    "Let's upgrade our simple Park environment to support 2D navigation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Park2D(GraphicEnvironment):\n",
    "    def percept(self, agent):\n",
    "        '''SENSOR FUNCTION: Return objects at agent's current [x,y] location'''\n",
    "        things = self.list_things_at(agent.location)\n",
    "        return things\n",
    "    \n",
    "    def execute_action(self, agent, action):\n",
    "        '''ACTUATOR FUNCTION: Process agent actions in 2D space'''\n",
    "        if action == \"move down\":\n",
    "            print('{} decided to {} at location: {}'.format(str(agent)[1:-1], action, agent.location))\n",
    "            agent.movedown()  # Move in Y direction\n",
    "        elif action == \"eat\":\n",
    "            items = self.list_things_at(agent.location, tclass=Food)\n",
    "            if len(items) != 0:\n",
    "                if agent.eat(items[0]):\n",
    "                    print('{} ate {} at location: {}'\n",
    "                          .format(str(agent)[1:-1], str(items[0])[1:-1], agent.location))\n",
    "                    self.delete_thing(items[0])  # Remove consumed food\n",
    "        elif action == \"drink\":\n",
    "            items = self.list_things_at(agent.location, tclass=Water)\n",
    "            if len(items) != 0:\n",
    "                if agent.drink(items[0]):\n",
    "                    print('{} drank {} at location: {}'\n",
    "                          .format(str(agent)[1:-1], str(items[0])[1:-1], agent.location))\n",
    "                    self.delete_thing(items[0])  # Remove consumed water\n",
    "                    \n",
    "    def is_done(self):\n",
    "        '''TERMINATION: End when no resources remain or agent dies'''\n",
    "        no_edibles = not any(isinstance(thing, Food) or isinstance(thing, Water) for thing in self.things)\n",
    "        dead_agents = not any(agent.is_alive() for agent in self.agents)\n",
    "        return dead_agents or no_edibles\n",
    "\n",
    "class BlindDog(Agent):\n",
    "    location = [0,1]              # 2D coordinate [x, y] position\n",
    "    direction = Direction(\"down\") # Track which way agent is facing\n",
    "    \n",
    "    def movedown(self):\n",
    "        # Move in positive Y direction (down in 4th quadrant)\n",
    "        self.location[1] += 1\n",
    "        \n",
    "    def eat(self, thing):\n",
    "        '''Consume food if present at current location'''\n",
    "        if isinstance(thing, Food):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def drink(self, thing):\n",
    "        '''Consume water if present at current location'''\n",
    "        if isinstance(thing, Water):\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing 2D Environment with Visual Feedback\n",
    "\n",
    "Now let's test our upgraded 2D park with the same dog behavior, but enhanced with **color-coded visualization**:\n",
    "- **Red (200,0,0)**: Our blind dog agent\n",
    "- **Blue (0,200,200)**: Water resources  \n",
    "- **Orange (230,115,40)**: Food resources\n",
    "\n",
    "This visual feedback helps us understand agent behavior patterns and environment dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE 2D ENVIRONMENT: 5x20 grid with color-coded visualization\n",
    "park = Park2D(5,20, color={'BlindDog': (200,0,0), 'Water': (0, 200, 200), 'Food': (230, 115, 40)}) \n",
    "dog = BlindDog(program)       # Use same decision-making program\n",
    "dogfood = Food()\n",
    "water = Water()\n",
    "\n",
    "# STRATEGIC PLACEMENT: Resources along the dog's linear path\n",
    "park.add_thing(dog, [0,1])       # Start at [0,1] \n",
    "park.add_thing(dogfood, [0,5])   # Food directly ahead at [0,5]\n",
    "park.add_thing(water, [0,7])     # Water further along at [0,7] \n",
    "morewater = Water()\n",
    "park.add_thing(morewater, [0,15]) # Additional water at [0,15]\n",
    "\n",
    "print(\"BlindDog starts at (0,1) facing downwards, let's see if he can find any food!\")\n",
    "park.run(20)  # Longer simulation for 2D environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitation Analysis: Why Our Dog Needs Better Navigation\n",
    "\n",
    "**Observation**: The graphics clearly show our blind dog **isn't utilizing the 2D space effectively** - it only moves in straight lines!\n",
    "\n",
    "**Problem Identified**: Our current agent has a **limited action repertoire** - it can only move \"down\" but cannot:\n",
    "- Turn left or right\n",
    "- Move in different directions  \n",
    "- Explore the full 2D environment\n",
    "- Handle obstacles or boundaries\n",
    "\n",
    "### Solution: Upgrade to EnergeticBlindDog\n",
    "\n",
    "Let's enhance our agent with **multi-directional navigation capabilities**:\n",
    "\n",
    "1. **Rotation Actions**: Turn left/right to change facing direction\n",
    "2. **Forward Movement**: Move in the currently facing direction\n",
    "3. **Collision Detection**: Respond to boundary encounters with direction changes\n",
    "4. **Stochastic Behavior**: Random exploration when no resources are detected\n",
    "\n",
    "This represents an evolution from **Simple Reflex Agent** to **Randomized Agent** with enhanced mobility.\n",
    "\n",
    "### New Behavioral Strategy\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><b>Percept:</b> </td>\n",
    "        <td>Feel Food </td>\n",
    "        <td>Feel Water</td>\n",
    "        <td>Feel Nothing</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "       <td><b>Action:</b> </td>\n",
    "       <td>eat</td>\n",
    "       <td>drink</td>\n",
    "       <td>\n",
    "       <table>\n",
    "           <tr>\n",
    "               <td><b>Situation:</b></td>\n",
    "               <td>At Boundary</td>\n",
    "               <td>Free Space</td>\n",
    "           </tr>\n",
    "           <tr>\n",
    "               <td><b>Action Distribution:</b></td>\n",
    "               <td>Turn Left (50%) / Turn Right (50%)</td>\n",
    "               <td>Turn Left (25%) / Turn Right (25%) / Move Forward (50%)</td>\n",
    "           </tr>\n",
    "       </table>\n",
    "       </td>\n",
    "   </tr>\n",
    "</table>\n",
    "\n",
    "**Key Innovation**: **Probabilistic action selection** introduces **exploration behavior** while **collision avoidance** prevents the agent from getting stuck at boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "class EnergeticBlindDog(Agent):\n",
    "    location = [0,1]              # Starting position in 2D space\n",
    "    direction = Direction(\"down\") # Current facing direction\n",
    "    \n",
    "    def moveforward(self, success=True):\n",
    "        '''ENHANCED MOBILITY: Move forward in currently facing direction\n",
    "        Args:\n",
    "            success: Whether movement is allowed (used for collision prevention)\n",
    "        '''\n",
    "        if not success:\n",
    "            return  # Collision prevention - don't move if blocked\n",
    "            \n",
    "        # DIRECTIONAL MOVEMENT: Update position based on current orientation\n",
    "        if self.direction.direction == Direction.R:      # Moving Right\n",
    "            self.location[0] += 1\n",
    "        elif self.direction.direction == Direction.L:    # Moving Left  \n",
    "            self.location[0] -= 1\n",
    "        elif self.direction.direction == Direction.D:    # Moving Down\n",
    "            self.location[1] += 1\n",
    "        elif self.direction.direction == Direction.U:    # Moving Up\n",
    "            self.location[1] -= 1\n",
    "    \n",
    "    def turn(self, d):\n",
    "        '''ROTATION CAPABILITY: Change facing direction'''\n",
    "        self.direction = self.direction + d\n",
    "        \n",
    "    def eat(self, thing):\n",
    "        '''Resource consumption behavior - unchanged from previous version'''\n",
    "        if isinstance(thing, Food):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def drink(self, thing):\n",
    "        '''Resource consumption behavior - unchanged from previous version'''\n",
    "        if isinstance(thing, Water):\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "def program(percepts):\n",
    "    '''ENHANCED AGENT PROGRAM: Implements probabilistic exploration with collision avoidance\n",
    "    \n",
    "    Decision Hierarchy:\n",
    "    1. SURVIVAL: Consume resources if available (highest priority)\n",
    "    2. AVOIDANCE: Turn when collision detected (safety)  \n",
    "    3. EXPLORATION: Random movement in free space (discovery)\n",
    "    '''\n",
    "    \n",
    "    # PRIORITY 1: Resource consumption (deterministic behavior)\n",
    "    for p in percepts:\n",
    "        if isinstance(p, Food):\n",
    "            return 'eat'    # Immediate food consumption\n",
    "        elif isinstance(p, Water):\n",
    "            return 'drink'  # Immediate water consumption\n",
    "        \n",
    "        # PRIORITY 2: Collision avoidance (safety behavior)\n",
    "        if isinstance(p,Bump):  # Boundary detected ahead\n",
    "            turn = False\n",
    "            choice = random.choice((1,2))  # 50-50 turn decision\n",
    "        else:\n",
    "            # PRIORITY 3: Exploration (stochastic behavior)\n",
    "            choice = random.choice((1,2,3,4))  # 25% left, 25% right, 50% forward\n",
    "            \n",
    "    # ACTION SELECTION: Convert choice to actual command\n",
    "    if choice == 1:\n",
    "        return 'turnright'   # Rotate clockwise\n",
    "    elif choice == 2:\n",
    "        return 'turnleft'    # Rotate counter-clockwise  \n",
    "    else:\n",
    "        return 'moveforward' # Continue in current direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENVIRONMENT - Enhanced Park2D\n",
    "\n",
    "Our environment must now support the agent's expanded action repertoire. Key enhancements include:\n",
    "\n",
    "1. **Predictive Collision Detection**: Check if next move would hit boundary\n",
    "2. **Multi-Action Processing**: Handle turn and movement commands\n",
    "3. **Boundary Feedback**: Provide \"Bump\" percepts when collision is imminent\n",
    "\n",
    "**Safety Design**: We prevent boundary violations rather than allowing them and recovering - this is more realistic for physical agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Park2D(GraphicEnvironment):\n",
    "    def percept(self, agent):\n",
    "        '''ENHANCED SENSOR FUNCTION: Provide location contents + collision warnings'''\n",
    "        things = self.list_things_at(agent.location)\n",
    "        \n",
    "        # PREDICTIVE COLLISION DETECTION: Check if forward movement would hit boundary\n",
    "        loc = copy.deepcopy(agent.location)  # Calculate target location\n",
    "        \n",
    "        # Determine next position based on current facing direction\n",
    "        if agent.direction.direction == Direction.R:\n",
    "            loc[0] += 1  # Moving right increases X\n",
    "        elif agent.direction.direction == Direction.L:\n",
    "            loc[0] -= 1  # Moving left decreases X\n",
    "        elif agent.direction.direction == Direction.D:\n",
    "            loc[1] += 1  # Moving down increases Y  \n",
    "        elif agent.direction.direction == Direction.U:\n",
    "            loc[1] -= 1  # Moving up decreases Y\n",
    "            \n",
    "        # BOUNDARY CHECK: Add Bump percept if next move would be out of bounds\n",
    "        if not self.is_inbounds(loc):\n",
    "            things.append(Bump())  # Warning: collision ahead!\n",
    "            \n",
    "        return things\n",
    "    \n",
    "    def execute_action(self, agent, action):\n",
    "        '''ENHANCED ACTUATOR FUNCTION: Process rotational and translational actions'''\n",
    "        if action == 'turnright':\n",
    "            # ROTATION ACTION: Change direction clockwise\n",
    "            print('{} decided to {} at location: {}'.format(str(agent)[1:-1], action, agent.location))\n",
    "            agent.turn(Direction.R)\n",
    "        elif action == 'turnleft':\n",
    "            # ROTATION ACTION: Change direction counter-clockwise  \n",
    "            print('{} decided to {} at location: {}'.format(str(agent)[1:-1], action, agent.location))\n",
    "            agent.turn(Direction.L)\n",
    "        elif action == 'moveforward':\n",
    "            # TRANSLATION ACTION: Move in facing direction\n",
    "            print('{} decided to move {}wards at location: {}'.format(str(agent)[1:-1], agent.direction.direction, agent.location))\n",
    "            agent.moveforward()\n",
    "        elif action == \"eat\":\n",
    "            # CONSUMPTION ACTION: Eat food at current location\n",
    "            items = self.list_things_at(agent.location, tclass=Food)\n",
    "            if len(items) != 0:\n",
    "                if agent.eat(items[0]):\n",
    "                    print('{} ate {} at location: {}'\n",
    "                          .format(str(agent)[1:-1], str(items[0])[1:-1], agent.location))\n",
    "                    self.delete_thing(items[0])  # Remove consumed food\n",
    "        elif action == \"drink\":\n",
    "            # CONSUMPTION ACTION: Drink water at current location\n",
    "            items = self.list_things_at(agent.location, tclass=Water)\n",
    "            if len(items) != 0:\n",
    "                if agent.drink(items[0]):\n",
    "                    print('{} drank {} at location: {}'\n",
    "                          .format(str(agent)[1:-1], str(items[0])[1:-1], agent.location))\n",
    "                    self.delete_thing(items[0])  # Remove consumed water\n",
    "                    \n",
    "    def is_done(self):\n",
    "        '''TERMINATION CONDITION: End simulation when resources exhausted or agent dies'''\n",
    "        no_edibles = not any(isinstance(thing, Food) or isinstance(thing, Water) for thing in self.things)\n",
    "        dead_agents = not any(agent.is_alive() for agent in self.agents)\n",
    "        return dead_agents or no_edibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Enhanced 2D Navigation\n",
    "\n",
    "Now let's observe our **EnergeticBlindDog** with full 2D mobility! This simulation will demonstrate:\n",
    "\n",
    "1. **Stochastic Exploration**: Random walk behavior in open areas\n",
    "2. **Collision Avoidance**: Boundary detection and evasion  \n",
    "3. **Resource Discovery**: Finding scattered food and water\n",
    "4. **Emergent Behavior**: Complex patterns from simple rules\n",
    "\n",
    "Watch how **random exploration** can effectively cover the 2D space, even without sophisticated pathfinding algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED 2D SIMULATION: Smaller space with scattered resources\n",
    "park = Park2D(5,5, color={'EnergeticBlindDog': (200,0,0), 'Water': (0, 200, 200), 'Food': (230, 115, 40)})\n",
    "dog = EnergeticBlindDog(program)  # Use enhanced agent with rotational capabilities\n",
    "\n",
    "# STRATEGIC RESOURCE DISTRIBUTION: Test 2D exploration ability\n",
    "dogfood = Food()\n",
    "water = Water()\n",
    "park.add_thing(dog, [0,0])        # Agent starts at corner [0,0]\n",
    "park.add_thing(dogfood, [1,2])    # Food requires 2D navigation to reach\n",
    "park.add_thing(water, [0,1])      # Water nearby for quick discovery\n",
    "morewater = Water()\n",
    "morefood = Food()\n",
    "park.add_thing(morewater, [2,4])  # Distant water tests persistence  \n",
    "park.add_thing(morefood, [4,3])   # Far corner food tests exploration\n",
    "\n",
    "print(\"Enhanced dog started at [0,0], facing down. Let's see if 2D exploration finds the scattered resources!\")\n",
    "park.run(20)  # Extended simulation to observe exploration patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Questions\n",
    "\n",
    "**Before proceeding to the Wumpus World, consider these questions:**\n",
    "\n",
    "1. **Efficiency vs. Exploration**: How does random exploration compare to systematic search? What are the trade-offs?\n",
    "\n",
    "2. **Scalability**: How would our agent perform in larger environments (100x100 grid)? \n",
    "\n",
    "3. **Learning**: Could our agent improve its performance by remembering visited locations?\n",
    "\n",
    "4. **Cooperation**: How might multiple agents coordinate in the same environment?\n",
    "\n",
    "5. **Real-world Applications**: Where do you see similar random exploration strategies used in robotics or AI systems?\n",
    "\n",
    "---\n",
    "\n",
    "## Wumpus Environment\n",
    "\n",
    "### Introduction to a Classic AI Challenge\n",
    "\n",
    "The **Wumpus World** is one of the most famous environments in AI literature, introduced in Russell & Norvig's textbook. It represents a significant step up in complexity from our simple park environment:\n",
    "\n",
    "**New Challenges:**\n",
    "- **Partial Observability**: Agent cannot see the entire environment\n",
    "- **Danger**: Deadly pits and the Wumpus creature can kill the agent\n",
    "- **Uncertainty**: Indirect evidence (stench, breeze) must be reasoned about\n",
    "- **Risk Assessment**: Agent must balance exploration vs. safety\n",
    "- **Goal-Oriented**: Specific objective (find gold and return safely)\n",
    "\n",
    "**Why Important**: This environment requires **logical reasoning**, **knowledge representation**, and **planning under uncertainty** - core AI capabilities beyond simple reactive behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipythonblocks import BlockGrid\n",
    "from agents import *\n",
    "\n",
    "# WUMPUS WORLD COLOR SCHEME: Visual representation of environment elements  \n",
    "color = {\"Breeze\": (225, 225, 225),    # Light gray - indicates nearby pit\n",
    "        \"Pit\": (0,0,0),                # Black - deadly obstacle\n",
    "        \"Gold\": (253, 208, 23),         # Bright yellow - goal object\n",
    "        \"Glitter\": (253, 208, 23),      # Yellow - indicates gold nearby\n",
    "        \"Wumpus\": (43, 27, 23),         # Dark brown - dangerous creature\n",
    "        \"Stench\": (128, 128, 128),      # Gray - indicates Wumpus nearby  \n",
    "        \"Explorer\": (0, 0, 255),        # Blue - our brave agent\n",
    "        \"Wall\": (44, 53, 57)            # Dark gray - environment boundary\n",
    "        }\n",
    "\n",
    "def program(percepts):\n",
    "    '''INTERACTIVE AGENT PROGRAM: Human-controlled decision making\n",
    "    This allows you to manually control the agent and experience the challenges\n",
    "    of reasoning under uncertainty in the Wumpus World'''\n",
    "    print(percepts)  # Display current sensory information\n",
    "    return input()   # Wait for human input (action command)\n",
    "\n",
    "# CREATE WUMPUS ENVIRONMENT: 7x7 grid world with hidden dangers\n",
    "w = WumpusEnvironment(program, 7, 7)         \n",
    "grid = BlockGrid(w.width, w.height, fill=(123, 234, 123))  # Green background\n",
    "\n",
    "def draw_grid(world):\n",
    "    '''VISUALIZATION FUNCTION: Render current world state as colored grid'''\n",
    "    global grid\n",
    "    grid[:] = (123, 234, 123)  # Reset to background color\n",
    "    \n",
    "    # Color each cell based on its contents\n",
    "    for x in range(0, len(world)):\n",
    "        for y in range(0, len(world[x])):\n",
    "            if len(world[x][y]):\n",
    "                # Use color of topmost object in each cell\n",
    "                grid[y, x] = color[world[x][y][-1].__class__.__name__]\n",
    "\n",
    "def step():\n",
    "    '''SIMULATION STEP: Update display and advance one time step'''\n",
    "    global grid, w\n",
    "    draw_grid(w.get_world())  # Render current state\n",
    "    grid.show()               # Display visual grid\n",
    "    w.step()                  # Process one agent action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START WUMPUS WORLD SIMULATION\n",
    "# Execute this cell to begin your adventure!\n",
    "# Available actions: 'Forward', 'TurnLeft', 'TurnRight', 'Shoot', 'Grab', 'Climb'\n",
    "step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wumpus World Challenge Instructions\n",
    "\n",
    "**Your Mission**: Navigate the dangerous cave system, find the gold, and return safely to the entrance.\n",
    "\n",
    "**Available Actions:**\n",
    "- `Forward` - Move one square in the facing direction\n",
    "- `TurnLeft` - Rotate 90° counter-clockwise  \n",
    "- `TurnRight` - Rotate 90° clockwise\n",
    "- `Shoot` - Fire arrow in facing direction (kills Wumpus, only one arrow available)\n",
    "- `Grab` - Pick up gold if present in current square\n",
    "- `Climb` - Exit cave (only works at entrance square [1,1])\n",
    "\n",
    "**Percepts and Their Meanings:**\n",
    "- **Stench**: Wumpus is in an adjacent square (dangerous!)\n",
    "- **Breeze**: Pit is in an adjacent square (deadly!)  \n",
    "- **Glitter**: Gold is in current square (goal!)\n",
    "- **Bump**: Tried to move into wall (blocked)\n",
    "- **Scream**: Your arrow killed the Wumpus (success!)\n",
    "\n",
    "**Strategic Thinking**: Use logical reasoning to deduce safe squares. For example:\n",
    "- If you sense a breeze, adjacent squares may contain pits\n",
    "- If no breeze, adjacent squares are guaranteed safe from pits\n",
    "- The Wumpus location can be triangulated using stench percepts from multiple positions\n",
    "\n",
    "**Challenge**: Try to complete the mission with minimal exploration - can you find the gold without visiting every square?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "Through this notebook, we've explored the fundamental concepts of intelligent agents:\n",
    "\n",
    "1. **Agent Architecture**: Understanding the sensor-actuator-program structure\n",
    "2. **Environment Design**: Creating worlds for agents to inhabit and interact with\n",
    "3. **Behavioral Evolution**: Progressing from simple reflex to probabilistic exploration\n",
    "4. **Dimensional Complexity**: Scaling from 1D to 2D navigation challenges  \n",
    "5. **Visual Feedback**: Using graphics to understand and debug agent behavior\n",
    "6. **Classic Challenges**: Experiencing the Wumpus World's reasoning requirements\n",
    "\n",
    "### Key AI Concepts Demonstrated\n",
    "\n",
    "- **Perception-Action Loop**: The fundamental cycle of intelligent behavior\n",
    "- **State Representation**: How agents track their position and orientation  \n",
    "- **Decision Making**: From deterministic rules to stochastic policies\n",
    "- **Collision Avoidance**: Safety mechanisms for physical agents\n",
    "- **Exploration vs. Exploitation**: Balancing discovery with resource utilization\n",
    "- **Reasoning Under Uncertainty**: Logic and inference in partially observable environments\n",
    "\n",
    "**Congratulations!** You now understand the core principles of intelligent agents and are ready to explore more advanced AI architectures and algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
